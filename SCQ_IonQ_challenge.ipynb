{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3d5ec2-72d6-436d-b1aa-52431e69292d",
   "metadata": {},
   "source": [
    "![IonQ Logo](./IonQCV/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f99ca2-f657-40d9-8382-691e5fa7c579",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# üëã Welcome to IonQ's **Quantum Computer Vision** challenge, hosted by SCQ!\n",
    "*by Willie Aboumrad, Vadim Karpusenko, and Heidi Nelson-Quillin*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef729124-e438-4c49-a120-021260f4062a",
   "metadata": {},
   "source": [
    "Image classification is a key task in computer vision, with researchers continually improving models to achieve higher accuracy on standardized benchmarks. Recently, quantum computing methods have also been introduced as potential solutions, or part of these.\n",
    "\n",
    "In this challenge, you'll take on the role of an IonQ Apps Team scientist, exploring image classification through quantum machine learning (QML) techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185cb52b-e9a9-4a17-9d19-f8f5a255392c",
   "metadata": {},
   "source": [
    "<video controls src=\"./IonQCV/ion_trap_animation.mp4\" width=90%; autoplay> </video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ff71b-68cb-45fe-8f2d-141f8ab2a5a5",
   "metadata": {},
   "source": [
    "We will tackle a binary classification problem, challenging you to design a hybrid classical-quantum neural network aimed at achieving the highest possible classification accuracy.\n",
    "\n",
    "To assist you, we‚Äôve provided exclusive access to unreleased features of IonQ‚Äôs next-generation SDK, which will handle the orchestration and the classical components of the hybrid pipeline. This allows you to concentrate on the most intriguing, purely quantum tasks:\n",
    "\n",
    "- üñºÔ∏è Encoding image data onto a quantum backend using an encoding circuit,\n",
    "- ‚öõÔ∏èüß© Differentiating encoded state vectors via a trainable ansatz, and\n",
    "- üìä Extracting relevant features for classification by measuring a set of observables.\n",
    "\n",
    "Before you begin, remember to activate the \"IonQ Vision Challenge\" environment on the qBraid platform and switch to that kernel. If you encounter any issues at this stage, do not hesitate to ask for help.\n",
    "\n",
    "![ENVS](./IonQCV/qBraid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f214ec9-0c03-4cf9-bbfe-0f7b0ddc880f",
   "metadata": {},
   "source": [
    "# üïµÔ∏è‚Äç‚ôÇÔ∏è Your mission, should you choose to accept it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a6784-aea4-4c6e-bd96-58d970677dd7",
   "metadata": {},
   "source": [
    "In this challenge, you'll build a hybrid quantum-classical image classification pipeline using <span style=\"color:purple\">ionqvision</span>, IonQ‚Äôs next-generation platform for QML computations, along with [PyTorch](https://pytorch.org/). Specifically, you'll train a quantum-classical neural network to differentiate between two distinct types of handwritten digits from the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf6d1f-0e1f-4b00-b73c-024773204968",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>üí° Tip:</b> If you're new to <span style=\"color:purple\">PyTorch</span>, no worries!\n",
    "\n",
    "This challenge is all about the **quantum** in [Quantum Machine Learning](https://en.wikipedia.org/wiki/Quantum_machine_learning). The provided starter code takes care of all the setup, allowing you to concentrate on designing your best quantum circuits!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c40429-b126-492e-b163-6d2d7e9ec631",
   "metadata": {},
   "source": [
    "The figure below showcases our hybrid pipeline, with the operations inside the gray box representing the quantum layer ‚Äî where all the magic happens.\n",
    "\n",
    "![hubrid neural network topology](./IonQCV/hybrid_nn.png)\n",
    "\n",
    "*<p style=\"text-align: center;\">Figure 1: A diagrammatic representation of our hybrid quantum-classical image classification pipeline</p>*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8648e6-b9b6-41b7-bbe5-67005b4e9412",
   "metadata": {},
   "source": [
    "Your mission, should you choose to accept it, is to design the quantum layer that achieves **the highest possible classification accuracy**, which will be automatically scored on an unseen validation set. Sounds simple, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76fead-7d15-4e06-9fdf-c977c2c8ea36",
   "metadata": {},
   "source": [
    "## üéª ‚öõ üéª Classical, quantum, and then classical again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfa813-525d-4421-89ee-1dd986772eeb",
   "metadata": {},
   "source": [
    "If you‚Äôve made it thus far, congratulations! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df996dd-8b8a-4ad9-8304-83ffe99c5c79",
   "metadata": {},
   "source": [
    "You're about to embark on an exciting journey of quantum exploration. To start, we‚Äôll dive into the architecture of our hybrid neural network. It's important to remember that running the full *hybrid* pipeline involves **both** classical and quantum hardware, along with software that enables communication between them. Fortunately, <span style=\"color:purple\">ionqvision</span> simplifies much of this complexity, allowing you to focus on the quantum design.\n",
    "\n",
    "In the following sections, we‚Äôll break down the classical and quantum components of the pipeline in more detail. After that, we‚Äôll guide you through the starter code so you can get up and running quickly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d47a3c-497f-43a2-a33b-68820119d765",
   "metadata": {},
   "source": [
    "### üéπ Classical pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7991eb-fd58-4bc1-9a1b-e95e029e26f7",
   "metadata": {},
   "source": [
    "The initial three layers of our pipeline are purely classical and perform standard operations commonly found in classical image classification workflows. First, the input black and white with 1 byte per pixel image (single-channel) is flattened - meaning all raws of image combined in 1D array, then we apply dimensionality reduction using [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis). Afterward, the compressed images are embedded into an even lower-dimensional feature space through a trainable fully-connected layer. During training, dropout is used, and the sigmoid [activation function](https://en.wikipedia.org/wiki/Activation_function) is applied.\n",
    "\n",
    "![Classical pre-processing neural network](./IonQCV/classical_pre.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b85af5-ffe8-4187-824d-431cfa0c8709",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><span style=\"color:black\">\n",
    "<b>‚ö†Ô∏è Note:</b> <br>\n",
    "This part of the architecture is essentially <b>fixed</b>: the only thing you get to decide here is the dimension \n",
    "<i>n</i> of the resulting latent vectors.<br> For instance, in <i>Figure 1</i>, <i>n = 12</i>.</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5ff47-c062-42ba-b285-57265a0d093a",
   "metadata": {},
   "source": [
    "### ‚öõÔ∏è Quantum Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e27c6-c26b-4991-9858-2b76a2fd2743",
   "metadata": {},
   "source": [
    "Next comes the most exciting part: the \"quantum layer.\" This is the heart of our challenge‚Äîyour design here will determine the highest classification accuracy, and there's a big prize for achieving that! ü•á\n",
    "\n",
    "The quantum layer is made up of three key components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f420a938-1920-4172-b5d1-14974abf4c1e",
   "metadata": {},
   "source": [
    "- An encoding circuit that loads each *n*-dimensional latent vector into an *n*-dimensional qubit state.\n",
    "- A variational circuit with *n* qubits and trainable parameters, designed to learn how to separate encoded latent vectors within the high-dimensional multi-qubit state space.\n",
    "- A set of *m* measurable quantities used to extract ‚Äúquantum features‚Äù from the transformed *n*-qubit states for classification. (For example, in *Figure 1*, *m = 4*.)\n",
    "\n",
    "![Quantum Layer](./IonQCV/quantum_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce79da7-816a-4215-8977-7cb6d40cd40f",
   "metadata": {},
   "source": [
    "To get started, explore the `ionqvision.ansatze.ansatz_library` module for inspiration. We've implemented several encoders and ansatze commonly used in the literature.\n",
    "\n",
    "For example, you might try using the `AngleEncoder` for your initial model, which has the following structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91010e0e-7d55-45bd-a230-0396d5772847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ansatz_library_custom\n",
    "from ansatz_library_custom import CustomEncoder\n",
    "\n",
    "encoder = CustomEncoder(num_qubits=4)\n",
    "encoder.draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d9405-785a-46f9-87d8-63744a6255f4",
   "metadata": {},
   "source": [
    "To view the function's description (docstring), simply run the code cell `AngleEncoder?`. To examine its implementation, open the file `ionqvision/ansatze/ansatz_library` and find the `AngleEncoder` class.\n",
    "\n",
    "Additionally, you can use its implementation as a template to create your own custom encoder designs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce873a7-7574-4820-bcf0-a0ae0a1d399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AngleEncoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e3512-6edd-4aa0-ba50-04aadb0de737",
   "metadata": {},
   "source": [
    "Similarly, you can utilize built-in options like the `BrickworkLayoutAnsatz` or the `QCNNAnsatz`, among others, for the trainable layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b87011-91ea-4737-9e76-8457ae8b185b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ansatz_library_custom\n",
    "from ansatz_library_custom import CustomAnsatz\n",
    "\n",
    "ansatz = CustomAnsatz(num_qubits=4)\n",
    "ansatz.draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f977b705-e700-4784-a75f-9db5fe0e824a",
   "metadata": {},
   "source": [
    "This ansatz is particularly intriguing as it features a sequence of \"convolution\" filters, interspersed with pooling operations that reduce the number of \"active\" qubits with each layer. Note that our implementation modifies [the original design](https://www.nature.com/articles/s41567-019-0648-8) by replacing mid-circuit measurements with controlled rotations.\n",
    "\n",
    "If you're interested in using this ansatz, be sure to [subclass](https://pybit.es/articles/python-subclasses/) and implement your own `QCNNAnsatz.ConvolutionBrickwork` and `QCNN.PoolingLayer` modules!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af5bcd-096a-4e49-a5d4-1aeb9c79fed6",
   "metadata": {},
   "source": [
    "```python\n",
    "class QCNNAnsatz(VariationalAnsatz):\n",
    "    r\"\"\"\n",
    "    Implement the Quantum Convolutional Network Ansatz (QCNN) as described in\n",
    "    :cite:t:`2019:qcnn`.\n",
    "\n",
    "    The quasi-local unitary $U_i$'s are entangling two-qubit gates with $6$\n",
    "    variational parameters.\n",
    "    They are laid out in a brickwork pattern with ``filter_depth`` layers.\n",
    "\n",
    "    The pooling operations are implemented by two-qubit controlled rotations,\n",
    "    with $2$ variational parameters.\n",
    "\n",
    "    The circuit starts with ``num_qubits`` active qubits and then half the\n",
    "    remaining qubits are discarded after each pooling operation until only a\n",
    "    single active qubit remains. This final qubit is measured and the result is\n",
    "    used for binary classification.\n",
    "    \"\"\"\n",
    "    class ConvolutionBrickwork(BrickworkLayoutAnsatz):\n",
    "        \"\"\"\n",
    "        Implement the convolution filters for the :class:`.QCNNAnsatz`.\n",
    "        \"\"\"\n",
    "        def __init__(self, num_qubits, num_layers, prefix=None, qubits=None, initial_state=None):\n",
    "            super().__init__(num_qubits, num_layers, blk_sz=3, prefix=prefix, qubits=qubits, initial_state=initial_state)\n",
    "        \n",
    "        def two_qubit_block(self, theta, q1, q2):\n",
    "            conv_op = QuantumCircuit(2, name=\"CONV\")\n",
    "            conv_op.ry(theta[0], 0)\n",
    "            conv_op.ry(theta[1], 1)\n",
    "            conv_op.rxx(theta[2], 0, 1)\n",
    "            self.append(conv_op.to_instruction(), [q1, q2])\n",
    "\n",
    "    class PoolingLayer(BrickworkLayoutAnsatz):\n",
    "        \"\"\"\n",
    "        Implement the pooling layer for the :class:`.QCNNAnsatz`.\n",
    "        \"\"\"\n",
    "        def __init__(self, num_qubits, prefix=None, qubits=None):\n",
    "            super().__init__(num_qubits, 1, blk_sz=1, prefix=prefix, qubits=qubits)\n",
    "    \n",
    "        def two_qubit_block(self, theta, q1, q2):\n",
    "            pool_op = QuantumCircuit(2, name=\"POOL\")\n",
    "            pool_op.crz(theta[0], 1, 0)\n",
    "            self.append(pool_op.to_instruction(), [q1, q2])\n",
    "\n",
    "    def __init__(self, num_qubits, filter_depth=2, initial_state=None):\n",
    "        num_layers = int(log(num_qubits, 2))\n",
    "        if abs(log(num_qubits, 2) - num_layers) > 1e-6:\n",
    "            raise ValueError(\"num_qubits must be a power of 2\")\n",
    "\n",
    "        super().__init__(num_qubits)\n",
    "        if initial_state is not None:\n",
    "            self.compose(initial_state, inplace=True)\n",
    "\n",
    "        for k in range(num_layers):\n",
    "            qubits = list(range(0, num_qubits, 2**k))\n",
    "        \n",
    "            conv = QCNNAnsatz.ConvolutionBrickwork(num_qubits, filter_depth, prefix=\"C\" + str(k), qubits=qubits)\n",
    "            self.compose(conv, inplace=True)\n",
    "            \n",
    "            pool = QCNNAnsatz.PoolingLayer(num_qubits, prefix=\"P\" + str(k), qubits=qubits)\n",
    "            self.compose(pool, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53722425-a474-4405-88c9-f68e22778862",
   "metadata": {},
   "source": [
    "Although <span style=\"color:purple\">ionqvision</span> doesn‚Äôt offer built-in qubit observables, you can configure them using <span style=\"color:purple\">Qiskit</span> with the following approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b372c-9aa0-4511-921a-a8ddf23e5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "# Measure the expectation value of X_0, Y_0, Z_0\n",
    "quantum_features = [\n",
    "    SparsePauliOp([\"IIIX\"]), \n",
    "    SparsePauliOp([\"IIIY\"]), \n",
    "    SparsePauliOp([\"IIIZ\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b1daa-da3e-4940-b9f6-36fb6d72de3e",
   "metadata": {},
   "source": [
    "Keep in mind that the encoder, ansatz, and quantum feature vector are closely interconnected: how you embed latent vectors into the (multi-)qubit state space should influence how you transform the encoded state vectors, which in turn should guide the features you choose to measure.\n",
    "\n",
    "The most effective model will likely leverage synergies from the deliberate co-design of these three components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d72b6af-60dc-48c1-bb82-fb0625f18607",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>‚ö°Ô∏è Important</b> <br>\n",
    "This is your chance to unleash your creativity! Think outside the box and show us what you can do. Don‚Äôt hesitate to tap into the vast resources available online and use everything at your disposal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841d092-f1d2-4c5d-b8d1-cdce9b336666",
   "metadata": {},
   "source": [
    "### üéπ Classical post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe7c81-78ce-4342-840b-44ed046a5c5a",
   "metadata": {},
   "source": [
    "After the quantum layer, the feature vectors in the classification pipeline are transferred back to the classical device for post-processing. Specifically, we train a fully-connected layer with a scalar output to minimize the binary [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy) between the final prediction and the true label of the input image. As before, dropout is applied during training, and the sigmoid [activation function](https://en.wikipedia.org/wiki/Activation_function) is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91926d-6804-4a5f-9829-a33bde257dfa",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"><img src=\"./IonQCV/classical_post.png\" alt=\"Classical post-processing\" width=\"50%\"/></p>\n",
    "\n",
    "This final stage is intentionally light, to ensure the quantum layer is the star of the show."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3a88a-3160-4375-a607-8012f1feb96c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><span style=\"color:black\">\n",
    "<b>‚ö†Ô∏è Note:</b> <br>\n",
    "This part of the architecture is completely <b>fixed</b>: the input dimension <i>(m)</i> is determined by the quantum feature vector, and the output is always a scalar.</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475974f0-980b-40f9-a0b1-a1b03bb4b0ce",
   "metadata": {},
   "source": [
    "## üöÇ Training your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ec770-2b91-4942-8392-db263a70fc6d",
   "metadata": {},
   "source": [
    "Once you've finalized your quantum layer, you can sit back and let <span style=\"color:purple\">ionqvision</span> handle the heavy lifting. Start by setting up your classifier and ensuring everything functions as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a186044-c533-48a1-b331-7a07ad01b38b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ionqvision.modules import BinaryMNISTClassifier\n",
    "\n",
    "# Set up your classifier and inspect its architecture\n",
    "classifier = BinaryMNISTClassifier(encoder, ansatz, quantum_features); classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188669d-f736-4283-b16c-45eb1b57bf4d",
   "metadata": {},
   "source": [
    "By default, `qiskit` will print numerous `INFO` messages about passed tests. To suppress these messages, run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c7ad6-9fb5-481b-9621-4e84a668fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf31df8-ee9f-4d65-91f4-2482d413da6a",
   "metadata": {},
   "source": [
    "If you're wondering, `BinaryMNISTClassifier` is a standard `torch.nn.Module`. At this stage, all the parameters in your quantum layer have been registered with `Torch`, and autograd will automatically compute the necessary gradients during the backward pass. There's nothing else to worry about!\n",
    "\n",
    "Make sure to check out the IonQ Vision Docs for more details on the inner workings of `BinaryMNISTClassifier` and related classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5969aee-d68c-402b-a597-541724b9e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out your quantum layer\n",
    "classifier.quantum_layer.layer_qc.draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be2c86-8c51-464c-bdf8-01f35622beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the images loaded correctly\n",
    "classifier.visualize_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4371dc-e94e-42cd-a893-d8c9edd55686",
   "metadata": {},
   "source": [
    "Now train your model. Use the `config` dictionary to control lower-level aspects of the training, like the number of `epochs`, the learning rate `lr`, the `betas` used by [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd37b6-101c-47df-be6b-736dd571ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Get a (pre-processed) training/val and test set\n",
    "train_val_set, test_set = classifier.get_train_test_set(train_size=400, test_size=100)\n",
    "train_set, val_set = random_split(train_val_set, [300, 100])\n",
    "train_set = train_set.dataset\n",
    "val_set = val_set.dataset\n",
    "\n",
    "# Configure model training hyper parameters\n",
    "config = {\n",
    "    \"epochs\": 4,\n",
    "    \"lr\": 0.1,\n",
    "    \"batch_size\": 50,\n",
    "    \"betas\": (0.9, 0.99),\n",
    "    \"weight_decay\": 1e-3,\n",
    "    \"clip_grad\": True,\n",
    "    \"log_interval\": 6,\n",
    "}\n",
    "\n",
    "# Train and plot the results\n",
    "classifier.train_module(train_set, val_set, config)\n",
    "classifier.plot_training_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8041ff-20d2-42b3-8741-5b20d3da5bfc",
   "metadata": {},
   "source": [
    "The model achieved '55.00%' on the training set and '58.00%' on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95898353-9d43-4267-8082-423347073ff7",
   "metadata": {},
   "source": [
    "## Evaluate Your Model\n",
    "Now that your model is trained, it's time to test! \n",
    "Run your trained model on the testing set using the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf8635-6ea3-46ac-9fee-daa01bc761fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_accuracy = classifier.compute_accuracy(test_set)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b4fd5-af68-4371-946d-f285c590f682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [ionqvision]",
   "language": "python",
   "name": "python3_ionq_v_u41rxp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
